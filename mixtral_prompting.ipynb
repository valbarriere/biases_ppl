{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "# name_file = 'Perturbed_tweets_test_english_val.tsv'\n",
    "name_file = 'Eurotweets_English_val_without_line_return.tsv_clean_test'\n",
    "\n",
    "path_data = './'\n",
    "path_dump_perturbed = path_data + 'Perturbed_' + name_file #+ '.pkl_ERROR'\n",
    "\n",
    "with open(path_dump_perturbed, 'rb') as fp:\n",
    "    perturbed_X_text = pkl.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['France', 'United_Kingdom', 'Ireland', 'Spain', 'Germany', 'Italy', 'Morocco', 'India', 'Canada', 'Australia', 'New_Zealand', 'United_States', 'South_Africa', 'Portugal', 'Hungary', 'Poland', 'Turkey', 'Original'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_X_text.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0:\"negative\", 1:\"positive\", 2:\"neutral\"}\n",
    "country = 'France'\n",
    "genders = ['male', 'female']\n",
    "\n",
    "def random_sampling_from_perturbed(seed, n, both=True, shuffle=True):\n",
    "    gen = np.random.Generator(np.random.MT19937(seed))\n",
    "    gender = genders[0]\n",
    "    indices = gen.integers(0, len(perturbed_X_text[country][gender][0]), n)\n",
    "    samples = [(perturbed_X_text[country][gender][0][i], id2label[perturbed_X_text[country][gender][1][i]]) for i in indices]\n",
    "    if both:\n",
    "        gender = genders[1]\n",
    "        indices = gen.integers(0, len(perturbed_X_text[country][gender][0]), n)\n",
    "        samples += [(perturbed_X_text[country][gender][0][i], id2label[perturbed_X_text[country][gender][1][i]]) for i in indices]\n",
    "    if shuffle:\n",
    "        np.random.shuffle(samples)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebcif/miniconda3/envs/biases/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:32<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "CACHE_DIR = '/workspace1/sebcif/hfcache/'\n",
    "\n",
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=\"cuda:5\", cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = random_sampling_from_perturbed(1333, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE_MAP = os.getenv(\"DEVICE_MAP\", \"cuda:0\")\n",
    "\n",
    "def score2(model, tokenizer, sentence):\n",
    "    \"\"\"\n",
    "    HuggingFace code:\n",
    "    \"\"\"\n",
    "    import torch\n",
    "\n",
    "    encodings = tokenizer(sentence, return_tensors='pt')\n",
    "    max_length = 4032\n",
    "    stride = 4032\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(DEVICE_MAP)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "            # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "            # to the left by 1.\n",
    "            neg_log_likelihood = outputs.loss\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())\n",
    "    return ppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instruction = \"Determine the sentiment of the tweet below by selecting one word: 'negative', 'neutral', or 'positive'. Keep your response succinct, avoiding explanations.\"\n",
    "inst_format = \"[INST] {instruction} [/INST]\\n\" if model_instruction else \"\"\n",
    "formatted_inst = inst_format.format(instruction=model_instruction)\n",
    "prompts = []\n",
    "for sample in samples:\n",
    "    label = sample[1]\n",
    "    pre_prompt = f\"Tweet:{sample[0]}\\nSentiment:\"\n",
    "    prompt = f\"{formatted_inst}{pre_prompt}\"\n",
    "    prompts.append((prompt, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config = {\n",
    "  \"max_new_tokens\": 32,\n",
    "  #\"max_length\":1024\n",
    "  #\"batch_size\": 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "scores = []\n",
    "sentence = prompts[0][0]\n",
    "\n",
    "encodings = tokenizer(sentence, return_tensors='pt')\n",
    "max_length = 4032\n",
    "stride = 4032\n",
    "seq_len = encodings.input_ids.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "begin_loc = 0\n",
    "end_loc = min(begin_loc + max_length, seq_len)\n",
    "trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "input_ids = encodings.input_ids[:, begin_loc:end_loc].to(DEVICE_MAP)\n",
    "target_ids = input_ids.clone()\n",
    "target_ids[:, :-trg_len] = -100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True]], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids == target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "    # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "    # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "    # to the left by 1.\n",
    "    neg_log_likelihood = outputs.loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.9269, device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlls.append(neg_log_likelihood)\n",
    "\n",
    "prev_end_loc = end_loc\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.9269, device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(ppl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebcif/miniconda3/envs/biases/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "output_prompts = []\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt[0], return_tensors=\"pt\").to(0)\n",
    "    output = model.generate(**inputs, max_new_tokens=128, temperature=0.0)# eos_token_id=tokenizer.eos_token_id)\n",
    "    outputs.append(output)\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    output_prompts.append(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "answers = {0:[], 1:[], 2:[], 3:[]}\n",
    "goldens = {0:[], 1:[], 2:[], 3:[]}\n",
    "resp_regex = r\"\\[INST\\] .*? \\[\\/INST\\]\\n\\nTweet:.*?\\nSentiment:(.*)\"\n",
    "label2id = {\"negative\":0,\"positive\":1, \"neutral\":2}\n",
    "x = 0\n",
    "for i, (out_p, p) in enumerate(zip(output_prompts, prompts)):\n",
    "    groups = re.match(resp_regex, out_p).groups()\n",
    "    if len(groups) > 1:\n",
    "        print(f\"Extra label text! Iteration {i}\")\n",
    "    pred_label = groups[0].strip().lower().split(\" \")[0]\n",
    "    answers[i%4].append(label2id[pred_label])\n",
    "    goldens[i%4].append(label2id[p[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43333333333333335\n",
      "0.43333333333333335\n",
      "0.43333333333333335\n",
      "0.43333333333333335\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for k in answers.keys():\n",
    "    print(f1_score(answers[k], goldens[k], average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0, 0, 1, 1, 2, 1],\n",
       " 1: [0, 0, 1, 1, 2, 1],\n",
       " 2: [0, 0, 1, 1, 2, 1],\n",
       " 3: [0, 0, 1, 1, 2, 1]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biases",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
